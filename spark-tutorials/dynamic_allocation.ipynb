{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/23 10:50:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ResourceAllocationApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10f0c2450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "\n",
    "spark = (\n",
    "        SparkSession\n",
    "            .builder\n",
    "            .appName('ResourceAllocationApp')\n",
    "            .master('local[*]')\n",
    "            \n",
    "            #.master('spark://C02D108AMD6W:7077')\n",
    "\n",
    "            .config('spark.dynamicAllocation.enabled', 'true')\n",
    "\n",
    "            .config('spark.sql.adaptive.enabled', 'false')\n",
    "            \n",
    "            .config('spark.sql.adaptive.skewJoin.enabled', 'false')\n",
    "\n",
    "\n",
    "            \n",
    "            #.config('spark.dynamicAllocation.shuffleTracking.enabled', 'true')\n",
    "#\n",
    "            #\n",
    "            #.config('spark.executor.memory', '2g')\n",
    "            #.config('spark.executor.cores', '2')\n",
    "#\n",
    "            #.config('spark.executor.instances', '2')\n",
    "#\n",
    "            #.config('spark.dynamicAllocation.minExecutors', '0')\n",
    "            #.config('spark.dynamicAllocation.maxExecutors', '5')\n",
    "#\n",
    "            #.config('spark.dynamicAllocation.schedulerBacklogTimeout', '1s')\n",
    "            #.config('spark.dynamicAllocation.executorIdleTimeout', '10s')\n",
    "            #.config('spark.dynamicAllocation.cachedExecutorIdleTimeout', '10s')\n",
    "            \n",
    "            .getOrCreate()\n",
    "\n",
    "    )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "productsDf = (\n",
    "    spark   \n",
    "        .range(1, 2000001)\n",
    "        .select(\n",
    "            F.col('id').alias('ProductId'),\n",
    "            F.expr('ROUND(RAND() * 100, 2) AS Price')\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "productsDf.createOrReplaceTempView('Products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesDf = (\n",
    "    spark\n",
    "        .range(1, 100000001)\n",
    "        .select(\n",
    "            F.col('id').alias('SalesId'),\n",
    "            F.expr(\n",
    "                \"\"\"\n",
    "                    CASE WHEN RAND() < 0.7\n",
    "                        THEN 1\n",
    "                    ELSE\n",
    "                        CAST (RAND() * 2000000 AS INT)\n",
    "                        END\n",
    "                \"\"\"\n",
    "            ).alias('ProductId'),\n",
    "            F.expr('CAST(RAND() * 10 AS INTEGER)').alias('QuantitySold'),\n",
    "            F.expr('DATE_ADD(CURRENT_DATE(), - CAST(RAND() * 365 AS INT))').alias('SalesState')\n",
    "\n",
    "        )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesDf.createOrReplaceTempView('Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"\"\"\n",
    "#    SELECT ProductId, COUNT(*) AS ProductCount\n",
    "#          FROM Sales\n",
    "#          Group By ProductId\n",
    "#          ORDER BY ProductCount DESC\n",
    "#\n",
    "#\n",
    "#\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_taxi_df = (\n",
    "    spark.read\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .csv('../data/YellowTaxis_202210.csv')\n",
    ")\n",
    "\n",
    "yellow_taxi_df_grouped =  (yellow_taxi_df.dropDuplicates()\n",
    "    .groupBy('PULocationID')\n",
    "    .agg(sum('total_amount')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|PULocationID| sum(total_amount)|\n",
      "+------------+------------------+\n",
      "|         148| 795087.6300000009|\n",
      "|         243|16739.699999999986|\n",
      "|          31| 935.1500000000001|\n",
      "|         137| 762800.5900000011|\n",
      "|          85|3172.5600000000004|\n",
      "|         251|            521.63|\n",
      "|          65| 80190.23000000001|\n",
      "|         255| 51031.78000000001|\n",
      "|          53|1601.2899999999995|\n",
      "|         133|3281.4800000000023|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/18 21:14:45 WARN TransportChannelHandler: Exception in connection from C02D108AMD6W/192.168.81.116:7077\n",
      "java.io.IOException: Operation timed out\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:47)\n",
      "\tat java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:330)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:284)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:259)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:417)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:259)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/09/18 21:14:45 WARN StandaloneAppClient$ClientEndpoint: Could not connect to C02D108AMD6W:7077: java.io.IOException: Operation timed out\n",
      "23/09/18 21:14:45 WARN StandaloneAppClient$ClientEndpoint: Could not connect to C02D108AMD6W:7077: java.io.IOException: Operation timed out\n",
      "23/09/18 21:14:45 WARN StandaloneAppClient$ClientEndpoint: Connection to C02D108AMD6W:7077 failed; waiting for master to reconnect...\n",
      "23/09/18 21:14:45 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "23/09/18 21:14:45 WARN StandaloneAppClient$ClientEndpoint: Connection to C02D108AMD6W:7077 failed; waiting for master to reconnect...\n"
     ]
    }
   ],
   "source": [
    "yellow_taxi_df_grouped.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
